@inproceedings{benabdeslemConstrainedLaplacianScore2011,
  title = {Constrained {{Laplacian Score}} for {{Semi-supervised Feature Selection}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Benabdeslem, Khalid and Hindawi, Mohammed},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {204--218},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23780-5_23},
  abstract = {In this paper, we address the problem of semi-supervised feature selection from high-dimensional data. It aims to select the most discriminative and informative features for data analysis. This is a recent addressed challenge in feature selection research when dealing with small labeled data sampled with large unlabeled data in the same set. We present a filter based approach by constraining the known Laplacian score. We evaluate the relevance of a feature according to its locality preserving and constraints preserving ability. The problem is then presented in the spectral graph theory framework with a study of the complexity of the proposed algorithm. Finally, experimental results will be provided for validating our proposal in comparison with other known feature selection methods.},
  isbn = {978-3-642-23780-5},
  langid = {english},
  keywords = {Constraints,Feature selection,Laplacian score},
  file = {/Users/fabiogadegbeku/Zotero/storage/HNVSGLUY/Benabdeslem and Hindawi - 2011 - Constrained Laplacian Score for Semi-supervised Fe.pdf}
}

@article{benabdeslemEfficientSemiSupervisedFeature2014,
  title = {Efficient {{Semi-Supervised Feature Selection}}: {{Constraint}}, {{Relevance}}, and {{Redundancy}}},
  shorttitle = {Efficient {{Semi-Supervised Feature Selection}}},
  author = {Benabdeslem, Khalid and Hindawi, Mohammed},
  year = {2014},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {5},
  pages = {1131--1143},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2013.86},
  urldate = {2023-09-24}
}

@article{caiFeatureSelectionMachine2018,
  title = {Feature Selection in Machine Learning: {{A}} New Perspective},
  shorttitle = {Feature Selection in Machine Learning},
  author = {Cai, Jie and Luo, Jiawei and Wang, Shulin and Yang, Sheng},
  year = {2018},
  month = jul,
  journal = {Neurocomputing},
  volume = {300},
  pages = {70--79},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.11.077},
  urldate = {2023-09-24},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/T95D98LQ/Cai et al. - 2018 - Feature selection in machine learning A new persp.pdf}
}

@article{heutteSelectionSemisuperviseeAttributs2011,
  title = {{S{\'e}lection semi-supervis{\'e}e d'attributs : Application {\`a} la classification de textures couleur}},
  author = {Heutte, Laurent and Govaert, G{\'e}rard},
  year = {2011},
  langid = {french},
  file = {/Users/fabiogadegbeku/Zotero/storage/83YSYZYV/Heutte and Govaert - 2011 - Sélection semi-supervisée d’attributs  Applicatio.pdf}
}

@article{kalakechConstraintScoresSemisupervised2011,
  title = {Constraint Scores for Semi-Supervised Feature Selection: {{A}} Comparative Study},
  shorttitle = {Constraint Scores for Semi-Supervised Feature Selection},
  author = {Kalakech, Mariam and Biela, Philippe and Macaire, Ludovic and Hamad, Denis},
  year = {2011},
  month = apr,
  journal = {Pattern Recognition Letters},
  volume = {32},
  number = {5},
  pages = {656--665},
  issn = {01678655},
  doi = {10.1016/j.patrec.2010.12.014},
  urldate = {2023-09-24},
  abstract = {Recent feature selection scores using pairwise constraints (must-link and cannot-link) have shown better performances than the unsupervised methods and comparable to the supervised ones. However, these scores use only the pairwise constraints and ignore the available information brought by the unlabeled data. Moreover, these constraint scores strongly depend on the given must-link and cannot-link subsets built by the user. In this paper, we address these problems and propose a new semi-supervised constraint score that uses both pairwise constraints and local properties of the unlabeled data. Experiments using Kendall's coefficient and accuracy rates, show that this new score is less sensitive to the given constraints than the previous scores while providing similar performances.},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/GKTJL5WB/Kalakech et al. - 2011 - Constraint scores for semi-supervised feature sele.pdf}
}

@book{laneIntroductionStatistics2003,
  title = {Introduction to Statistics},
  author = {Lane, David and Scott, David and Hebl, Mikki and Guerra, Rudy and Osherson, Dan and Zimmer, Heidi},
  year = {2003},
  publisher = {{Citeseer}},
  file = {/Users/fabiogadegbeku/Zotero/storage/F94T55B5/Lane et al. - 2003 - Introduction to statistics.pdf}
}

@misc{mathieuMachineLearningLecture,
  title = {Machine {{Learning Lecture Notes}}},
  author = {Mathieu, Timothee},
  file = {/Users/fabiogadegbeku/Zotero/storage/5XH5IXSJ/Mathieu - Machine Learning Lecture Notes.pdf}
}

@article{salmiConstrainedFeatureSelection2021,
  title = {Constrained Feature Selection for Semisupervised Color-Texture Image Segmentation Using Spectral Clustering},
  author = {Salmi, Abderezak and Hammouche, Kamal and Macaire, Ludovic},
  year = {2021},
  month = feb,
  journal = {Journal of Electronic Imaging},
  volume = {30},
  number = {01},
  issn = {1017-9909},
  doi = {10.1117/1.JEI.30.1.013014},
  urldate = {2023-09-25},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/IFF36ZZ4/Salmi et al. - 2021 - Constrained feature selection for semisupervised c.pdf}
}

@article{salmiConstrainedFeatureSelection2021a,
  title = {Constrained Feature Selection for Semisupervised Color-Texture Image Segmentation Using Spectral Clustering},
  author = {Salmi, Abderezak and Hammouche, Kamal and Macaire, Ludovic},
  year = {2021},
  month = feb,
  journal = {Journal of Electronic Imaging},
  volume = {30},
  number = {01},
  issn = {1017-9909},
  doi = {10.1117/1.JEI.30.1.013014},
  urldate = {2023-10-10},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/TPBZEJQ5/Salmi et al. - 2021 - Constrained feature selection for semisupervised c.pdf}
}

@article{salmiSimilaritybasedConstraintScore2020,
  title = {Similarity-Based Constraint Score for Feature Selection},
  author = {Salmi, Abderezak and Hammouche, Kamal and Macaire, Ludovic},
  year = {2020},
  month = dec,
  journal = {Knowledge-Based Systems},
  volume = {209},
  pages = {106429},
  issn = {09507051},
  doi = {10.1016/j.knosys.2020.106429},
  urldate = {2023-09-23},
  abstract = {To avoid the curse of dimensionality resulting from a large number of features, the most relevant features should be selected. Several scores involving mustlink and cannot-link constraints have been proposed to estimate the relevance of features. However, these constraint scores evaluate features one by one and ignore any correlations between features. In addition, they compute distances in the high-dimensional original feature space to evaluate the similarity between samples. So, they would be corrupted by the curse of dimensionality. To deal with these drawbacks, we propose a new constraint score based on a similarity matrix that is computed in the selected feature subspace and that makes it possible to evaluate the relevance of a feature subset at once. Experiments on benchmark databases demonstrate the improvement brought by the proposed constraint score in the context of both supervised and semi-supervised learnings.},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/3MBCQT3C/Salmi et al. - 2020 - Similarity-based constraint score for feature sele.pdf}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  month = may,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781107298019},
  urldate = {2023-09-08},
  abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
  isbn = {978-1-107-05713-5 978-1-107-29801-9},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/FKZZFK6T/Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning From Theory to Alg.pdf}
}

@article{sheikhpourSurveySemisupervisedFeature2017,
  title = {A {{Survey}} on Semi-Supervised Feature Selection Methods},
  author = {Sheikhpour, Razieh and Sarram, Mehdi Agha and Gharaghani, Sajjad and Chahooki, Mohammad Ali Zare},
  year = {2017},
  month = apr,
  journal = {Pattern Recognition},
  volume = {64},
  pages = {141--158},
  issn = {00313203},
  doi = {10.1016/j.patcog.2016.11.003},
  urldate = {2023-09-24},
  abstract = {Feature selection is a significant task in data mining and machine learning applications which eliminates irrelevant and redundant features and improves learning performance. In many real-world applications, collecting labeled data is difficult, while abundant unlabeled data are easily accessible. This motivates researchers to develop semi-supervised feature selection methods which use both labeled and unlabeled data to evaluate feature relevance. However, till-to-date, there is no comprehensive survey covering the semisupervised feature selection methods. In this paper, semi-supervised feature selection methods are fully investigated and two taxonomies of these methods are presented based on two different perspectives which represent the hierarchical structure of semi-supervised feature selection methods. The first perspective is based on the basic taxonomy of feature selection methods and the second one is based on the taxonomy of semisupervised learning methods. This survey can be helpful for a researcher to obtain a deep background in semisupervised feature selection methods and choose a proper semi-supervised feature selection method based on the hierarchical structure of them.},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/XSDHVKVF/Sheikhpour et al. - 2017 - A Survey on semi-supervised feature selection meth.pdf}
}

@article{yangSemisupervisedMinimumRedundancy2018,
  title = {Semi-Supervised Minimum Redundancy Maximum Relevance Feature Selection for Audio Classification},
  author = {Yang, Xu -Kui and He, Liang and Qu, Dan and Zhang, Wei-Qiang},
  year = {2018},
  month = jan,
  journal = {Multimedia Tools and Applications},
  volume = {77},
  number = {1},
  pages = {713--739},
  issn = {1573-7721},
  doi = {10.1007/s11042-016-4287-0},
  urldate = {2023-09-24},
  abstract = {It is still a changing problem of choosing the most relevant ones from multiple features for their specific machine learning tasks. However, feature selection provides an effective solution to it, which aims to choose the most relevant and least redundant features for data analysis. In this paper, we present a feature selection algorithm termed as semi-supervised minimum redundancy maximum relevance. The relevance is measured by a semi-supervised filter score named constraint compensated Laplacian score, which takes advantage of the local geometrical structures of unlabeled data and constraint information from labeled data. The redundancy is measured by a semi-supervised Gaussian mixture model-based Bhattacharyya distance. The optimal feature subset is selected by maximizing feature relevance and minimizing feature redundancy simultaneously. We apply our algorithm in audio classification task and compare it with other known feature selection methods. Experimental results further prove that our algorithm can lead to promising improvements.},
  langid = {english},
  keywords = {Audio classification,Bhattacharyya distance,Constraint information,Locality preserving,Maximal relevance,Minimal redundancy,Semi-supervised feature selection},
  file = {/Users/fabiogadegbeku/Zotero/storage/JFGI5YHX/Yang et al. - 2018 - Semi-supervised minimum redundancy maximum relevan.pdf}
}

@article{zhangConstraintScoreNew2008,
  title = {Constraint {{Score}}: {{A}} New Filter Method for Feature Selection with Pairwise Constraints},
  shorttitle = {Constraint {{Score}}},
  author = {Zhang, Daoqiang and Chen, Songcan and Zhou, Zhi-Hua},
  year = {2008},
  month = may,
  journal = {Pattern Recognition},
  volume = {41},
  number = {5},
  pages = {1440--1451},
  issn = {00313203},
  doi = {10.1016/j.patcog.2007.10.009},
  urldate = {2023-09-24},
  abstract = {Feature selection is an important preprocessing step in mining high-dimensional data. Generally, supervised feature selection methods with supervision information are superior to unsupervised ones without supervision information. In the literature, nearly all existing supervised feature selection methods use class labels as supervision information. In this paper, we propose to use another form of supervision information for feature selection, i.e. pairwise constraints, which specifies whether a pair of data samples belong to the same class (must-link constraints) or different classes (cannot-link constraints). Pairwise constraints arise naturally in many tasks and are more practical and inexpensive than class labels. This topic has not yet been addressed in feature selection research. We call our pairwise constraints guided feature selection algorithm as Constraint Score and compare it with the well-known Fisher Score and Laplacian Score algorithms. Experiments are carried out on several high-dimensional UCI and face data sets. Experimental results show that, with very few pairwise constraints, Constraint Score achieves similar or even higher performance than Fisher Score with full class labels on the whole training data, and significantly outperforms Laplacian Score. ᭧ 2007 Elsevier Ltd. All rights reserved.},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/ZSUYNZZS/Zhang et al. - 2008 - Constraint Score A new filter method for feature .pdf}
}

@article{zhaoLocalitySensitiveSemisupervised2008,
  title = {Locality Sensitive Semi-Supervised Feature Selection},
  author = {Zhao, Jidong and Lu, Ke and He, Xiaofei},
  year = {2008},
  month = jun,
  journal = {Neurocomputing},
  volume = {71},
  number = {10-12},
  pages = {1842--1849},
  issn = {09252312},
  doi = {10.1016/j.neucom.2007.06.014},
  urldate = {2023-09-24},
  abstract = {In many computer vision tasks like face recognition and image retrieval, one is often confronted with high-dimensional data. Procedures that are analytically or computationally manageable in low-dimensional spaces can become completely impractical in a space of several hundreds or thousands dimensions. Thus, various techniques have been developed for reducing the dimensionality of the feature space in the hope of obtaining a more manageable problem. The most popular feature selection and extraction techniques include Fisher score, Principal Component Analysis (PCA), and Laplacian score. Among them, PCA and Laplacian score are unsupervised methods, while Fisher score is supervised method. None of them can take advantage of both labeled and unlabeled data points. In this paper, we introduce a novel semi-supervised feature selection algorithm, which makes use of both labeled and unlabeled data points. Specifically, the labeled points are used to maximize the margin between data points from different classes, while the unlabeled points are used to discover the geometrical structure of the data space. We compare our proposed algorithm with Fisher score and Laplacian score on face recognition. Experimental results demonstrate the efficiency and effectiveness of our algorithm.},
  langid = {english},
  file = {/Users/fabiogadegbeku/Zotero/storage/3D6LNP4W/Zhao et al. - 2008 - Locality sensitive semi-supervised feature selecti.pdf}
}
